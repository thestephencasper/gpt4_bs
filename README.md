# Prompts that cause ChatGPT-4 to hallucinate.

Contributors: Stephen Casper (scasper@mit.edu), Luke Bailey, Zachary Marinov, Michael Gerovich, Andrew Garber, Shuvom Sadhuka, Oam Patel, Riley Kong

We compiled 104 examples of prompts that cause ChatGPT-4 (May 12 and May 24 2023 versions) to hallucinate untrue content and grouped them into 18 categories. Here we are sharing the examples. 

The categories are 
- Arbitrarily resolving ambiguity
- Being asked to say false things
- BS about fictitious things
- BS about unremarkable things
- BS extrapolation from trends
- BS meanings of theorems
- BS proofs of true theorems
- BS uses of unrelated lemmas
- BS references
- Common misconceptions
- Defending BS
- Deferring to doubt
- Failing to answer all
- Failing to answer none
- Imitating untrustworthy people
- Justifying a wrong response
- Making up outrageous facts
- Shifts from a common setup

Enjoy!
