# Prompts that cause ChatGPT-4 to hallucinate.

Contributors: Stephen Casper (scasper@mit.edu), Luke Bailey (lukebailey@college.harvard.edu), Zachary Marinov (zmarinov@mit.edu), Michael Gerovich (mgerov@mit.edu), Andrew Garber (andrewgarber@college.harvard.edu), Shuvom Sadhuka (ssadhuka@mit.edu), Oam Patel (opatel@college.harvard.edu), Riley Kong (rileyis@mit.edu)

We compiled 104 examples of prompts that cause ChatGPT-4 (May 12 and May 24 2023 versions) to hallucinate untrue content and grouped them into 18 categories. Here we are sharing the examples. 

The categories are 
- Arbitrarily resolving ambiguity
- Being asked to say false things (this category is non-adversarial)
- BS about fictitious things
- BS about unremarkable things
- BS extrapolation from trends
- BS meanings of theorems
- BS proofs of true theorems
- BS uses of unrelated lemmas
- BS references
- Common misconceptions
- Defending BS
- Deferring to doubt
- Failing to answer all
- Failing to answer none
- Imitating untrustworthy people (this category is non-adversarial)
- Justifying a wrong response
- Making up outrageous facts
- Shifts from a common setup

Enjoy!
