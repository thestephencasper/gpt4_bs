# Prompts that cause ChatGPT-4 to hallucinate.

**Contributors:** Stephen Casper (scasper@mit.edu), Luke Bailey (lukebailey@college.harvard.edu), Zachary Marinov (zmarinov@mit.edu), Michael Gerovich (mgerov@mit.edu), Andrew Garber (andrewgarber@college.harvard.edu), Shuvom Sadhuka (ssadhuka@mit.edu), Oam Patel (opatel@college.harvard.edu), Riley Kong (rileyis@mit.edu)

**Dataset:** We compiled 52 examples of prompts that cause ChatGPT-4 (May 12 and May 24 2023 versions) to hallucinate untrue content and grouped them into 11 categories. Here we are sharing the examples. 

The categories are 
- BS about fictitious things
- BS about unremarkable things
- BS extrapolation from trends
- BS meanings of theorems
- BS proofs of true theorems
- BS uses of unrelated lemmas
- BS references
- Defending BS
- Deferring to doubt
- Justifying a wrong response
- Making up outrageous facts

**What we hope this is useful for:** Our dataset of examples is small and was collected with a just-messing-around methodology. But some might find that these examples make for decent ones to use for testing various behaviors of chatbots involving hallucination. Our taxonomy could also be useful for more systematically studying hallucination. We also invite OpenAI to fix these issues and for anyone with additional ideas or examples to send them to us so we can update the dataset :)

